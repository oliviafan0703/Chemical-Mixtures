---
title: "Tutorial 07 - Bayesian Model Averaging"
author: "Olivia Fan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    css: "./tutorials.css"
    toc: true
    toc_float: true
---

<!-- Setup -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning=FALSE,
                      message=FALSE)
```

## Background & Introduction 

Traditionally, analysis often proceeds by first selecting the best model according to some criterion and then learning about the parameters given that the selected model is the underlying truth. However, from the Bayesian perspective, an alternative is to learn the parameters for *all* candidate models and then combine the estimates according to the posterior probabilities of associated model. In this module, we will investigate this method known as *Bayesian Model Averaging*, and the advantages it has over all-or-none model selection. 

## Topics 

- Advantages of BMA over all-or-none model selection through the examples of analysis of covariance, meta-analysis and network analysis.

- Estimate the slope and intercept of the regression line using the least squares method.

- Interpret the slope and intercept of the regression line.

## 1. Definitions

### Bayes factor
The change from prior to posterior model odds brought about by the data; equivalently, how much more likely the observed data are under one model versus another.

### Bayesian model average
A parameter estimate (or a prediction of new observations) obtained by averaging the estimates (or predictions) of the different models under consideration, each weighted by its model probability.

## 2. Libraries

The R package `BMA` provides ways of carrying out BMA for linear regression, generalized linear models, and survival or event history analysis us- ing Cox proportional hazards models.

```{r}
library(BMA)
library(tidyverse)
```

## 3. Example : Linear Regression

To illustrate how BMA takes account of model uncertainty about the variables to be included in lin- ear regression, we will be using the ``nhanes`` dataset where the variables are described in the file `nhanes-codebook.txt`. Load this data with the `load` function and specify the data file. 

```{r}
load(file='nhanes1518.rda')
```

<!-- Experimentation with UScrime data: -->

<!-- ```{r} -->
<!-- UScrime<- MASS::UScrime -->
<!-- x.crime<- UScrime[,-16] -->
<!-- y.crime<- log(UScrime[,16]) -->
<!-- x.crime[,-2]<- log(x.crime[,-2]) -->
<!-- crime.bicreg <- bicreg(x.crime, y.crime) -->
<!-- summary (crime.bicreg, digits=2) -->
<!-- ``` -->

We first subset the dataset to include only the predictors in linear regressions module, in order to filter out some NA values:

```{r}
nhanes_na_removed<-cbind(nhanes1518[1:5],nhanes1518$BMXBMI)%>%
  rename(BMXBMI="nhanes1518$BMXBMI")%>%
  na.omit()
```

We fit a BMA model on the dataset:

```{r}
x.nhanes <-log(nhanes_na_removed[-6])
y.nhanes <-log(nhanes_na_removed[6])
y.nhanes<-as.numeric(unlist(y.nhanes))
nhanes.bicreg <- bicreg(x.nhanes, y.nhanes)
summary(nhanes.bicreg)
```


The BMA posterior distributions of all the parameters are produced by the command:

```{r}
plot (nhanes.bicreg,mfrow=c(3,2))
```


We see that the distribution of waist circumference is asymmetric, with peaks at age of around 80 and 0 respectively.

We can also add the mean line, as well as overlay with transparent density plot. The value of alpha controls the level of transparency:

## 4 Example: Logistic Regression

Now we illustrate BMA for logistic regression using the categorical variable `income` on the response BMI

```{r}
nhanes_na_removed<-cbind(nhanes1518[1:5],nhanes1518$BMXBMI,nhanes1518$RIDAGEYR,nhanes1518$INDHHIN2)%>%
  rename(BMXBMI="nhanes1518$BMXBMI")%>%
  rename(income = "nhanes1518$INDHHIN2")%>%
  rename(RIDAGEYR = "nhanes1518$RIDAGEYR")%>%
  na.omit()
# turn quantitative variable into categorical variable
nhanes_na_removed$income<-as.character(nhanes_na_removed$income)
```

Refer to the website for encoding of income categories:

https://wwwn.cdc.gov/nchs/nhanes/2011-2012/demo_g.htm#INDHHIN2

We want to first drop categories with values 77 (Refused) and 99 (Don't Know) first:
```{r}
nhanes_na_removed <- subset(nhanes_na_removed, income!="77" & income!="99")
```

Then we transform the variable `RIAGENDR` to be binary:
```{r}
nhanes_na_removed<-nhanes_na_removed%>%
  mutate(Gender_binary=RIAGENDR-1)
```

Then we fit the Bayesian logistic regression model on predictors including categorical variable `income`:

```{r}
nhanes.bic.glm<-bic.glm(Gender_binary~income+BMXBMI+WTINT2YR+WTMEC2YR, data=nhanes_na_removed, glm.family="binomial")
summary(nhanes.bic.glm)
plot (nhanes.bic.glm,mfrow=c(2,2))
summary(nhanes.bic.glm, conditional=T, digits=3)
```


## 5 Example: Survival Analysis

```{r}
nhanes_na_removed$WTINT2YR<-log(nhanes_na_removed$WTINT2YR)
nhanes_na_removed$WTMEC2YR<-log(nhanes_na_removed$WTMEC2YR)
nhanes_na_removed$BMXBMI<-log(nhanes_na_removed$BMXBMI)
nhanes_na_removed$WTINT2YR<-log(nhanes_na_removed$RIDAGEYR)
# nhanes.bic.surv <- bic.surv(BMXBMI,RIDAGEYR,data=nhanes_na_removed)
```


## Related Topics

| **Topics** | **Concepts** |
|-------------------------------------------------------------|:--:|
| ANOVA | Null hypothesis |
| Quantile factoring | use qualitative  as levels |
| Cross-validation 	|  k-fold, LOOCV	|
| Model section 	|  AIC, BIC, Cp	|

